{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_access_token = os.environ[\"HF_ACCESS_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(row):\n",
    "\n",
    "    template = \"\"\"Choose from A, B, C, or D to respond to the question below. Do not provide an explanation. Format your response as a JSON string:\n",
    "\n",
    "{{\n",
    "   \"answer\" : <single letter> \n",
    "}}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "A. {a}\n",
    "B. {b}\n",
    "C. {c}\n",
    "D. {d}\n",
    "\"\"\"\n",
    "\n",
    "    return template.format(question=row[\"question\"], a=row[\"A\"], b=row[\"B\"], c=row[\"C\"], d=row[\"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_outputs(question, model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    tokens = tokens.to(\"cuda\")\n",
    "\n",
    "    num_input_tokens = len(tokens[0])\n",
    "    generated_outputs = model.generate(tokens, max_new_tokens=50, do_sample=False, num_return_sequences=1, output_scores=True, output_hidden_states=True, return_dict_in_generate=True)\n",
    "    \n",
    "    return num_input_tokens, generated_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datapoints(num_input_tokens, outputs, tokenizer):\n",
    "    completion = tokenizer.decode(outputs[\"sequences\"][0][num_input_tokens:])\n",
    "\n",
    "    first_token_scores = outputs[\"scores\"][0].squeeze()\n",
    "    first_token_dist = F.softmax(first_token_scores)\n",
    "    input_final_state = outputs[\"hidden_states\"][0][-1]\n",
    "    \n",
    "    # squeeze out the batch dim\n",
    "    average_input_final_state = input_final_state.squeeze().mean(dim=0) \n",
    "\n",
    "    return completion, first_token_dist, average_input_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = {\n",
    "    \"original\": \"mmlu_200_orig.csv\",\n",
    "    \"singlechar\": \"mmlu_200_singlechar_perturbed.csv\",\n",
    "    \"reworded\": \"mmlu_200_gpt_perturbed.csv\"  \n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"meta-llama\": [\"Llama-2-7b-chat-hf\"],\n",
    "    \"mistralai\": [\"Mistral-7B-Instruct-v0.2\"],\n",
    "    \"google\": [\"gemma-7b-it\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_org in models:\n",
    "    for model_name in models[model_org]:\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(f\"{model_org}/{model_name}\", token=hf_access_token, quantization_config=quantization_config, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"{model_org}/{model_name}\", token=hf_access_token)\n",
    "        \n",
    "        # just run up to this part first to confirm can load\n",
    "\n",
    "        for experiment, filename in experiment_data.items():\n",
    "            print(f\"Proceeding with experiment '{experiment}' with model '{model_name}'...\")\n",
    "            df = pd.read_csv(f\"final_data/{filename}\")\n",
    "            \n",
    "            if os.path.isdir(f\"run_data/{model_name}/{experiment}\") == False:\n",
    "                os.makedirs(f\"run_data/{model_name}/{experiment}\", exist_ok=True)\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                paths = {\n",
    "                    \"completion\": f\"run_data/{model_name}/{experiment}/completion-{row['question_id']}\",\n",
    "                    \"dist\": f\"run_data/{model_name}/{experiment}/dist-{row['question_id']}\",\n",
    "                    \"hidden\": f\"run_data/{model_name}/{experiment}/hidden-{row['question_id']}\"\n",
    "                }\n",
    "\n",
    "                formatted_question = generate_question(row)\n",
    "\n",
    "                if idx % 25 == 0:\n",
    "                    print(formatted_question)\n",
    "\n",
    "                num_input_tokens, outputs = generate_all_outputs(formatted_question, model, tokenizer)\n",
    "                completion, first_token_dist, average_input_final_state = extract_datapoints(num_input_tokens, outputs, tokenizer)\n",
    "\n",
    "                with open(paths[\"completion\"], \"w\") as f:\n",
    "                    f.write(completion)\n",
    "\n",
    "                np.save(paths[\"dist\"], first_token_dist.cpu().numpy())\n",
    "                np.save(paths[\"hidden\"], average_input_final_state.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_and_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
